{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf830ffb",
   "metadata": {},
   "source": [
    "# Anomaly Detection via RandomCutForest\n",
    "\n",
    "Mike Ames  \n",
    "mikames@amazon.com  \n",
    "13APR2022  \n",
    "\n",
    "\n",
    "### Overview\n",
    "-----\n",
    "\n",
    "There are dozens (probably hundreds) of books dedicated to the subject of anomaly and outlier detection. The most common approaches expect that you know what “normal” looks like so that you can identify instances that don’t conform to the “normal” profile. There are statistical techniques, supervised, unsupervised, even semi-supervised methods to look at anomalies in data. This notebook uses Amazon SageMaker’s Random Cut Forest method to detect anomalous data points within a data set. RCF is an unsupervised algorithm which can be used to identify observations which diverge from otherwise well-structured data. \n",
    "\n",
    "RCF associates each data point with anomaly score. Low score values indicate that the data point is considered \"normal.\" High values indicate the presence of an anomaly in the data. The definitions of \"low\" and \"high\" depend on the dataset as well see. \n",
    "\n",
    "\n",
    "### How to use this notebook \n",
    "-----\n",
    "The goal of this notebook is to identify some number of anomalies in your data and use the RCF to create an EVENT_LABEL. In the event that you have some examples of fraud, great we'll perserve those if not we'll go ahead and gernate a EVENT_LABEL and assign a number of anomalies you can use as a target. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Setup \n",
    "-----\n",
    "To get started you need the following information \n",
    "1. location of your CSV file to train on. \n",
    "2. you'll need to fire up sagemaker and ensure that the sagemaker user is authorized to publish an endpiont \n",
    "3. location of where you want to write out your CSV file. \n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\"> <strong> Overview </strong>\n",
    "\n",
    "- S3_BUCKET: this is the location you will be reading and writing from \n",
    "- S3_FILE_IN: this is the input file name you'll use to train your RCF on.   \n",
    "- S3_FILE_OUT: this is the output file name you'll write your CSV file out with. \n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29a81e73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "from IPython.display import clear_output\n",
    "display(HTML(\"<style>.container { width:90% }</style>\"))\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "# -- common stuff -- \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from io import StringIO\n",
    "\n",
    "# -- AWS stuff --- \n",
    "import boto3\n",
    "import botocore\n",
    "import sagemaker\n",
    "import sys\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "# -- need this to render charts in notebook -- \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7f4cad",
   "metadata": {},
   "source": [
    "## Your Dataset and Location \n",
    "\n",
    "---- \n",
    "\n",
    "My dataset happens to be located in a S3 bucket, yours could be local just change the read_csv to the appropraite location. All we are going to do here is read the file into a panda's dataframe. Then \n",
    "\n",
    "<div class=\"alert alert-info\"> <strong> S3 </strong>\n",
    " \n",
    "- S3_BUCKET : name of the bucket containing the file \n",
    "- S3_FILE_IN   : name of the file you will train your RCF with, often known as the file_ or data_key\n",
    "- S3_FILE_OUT   : name of the file you will create with your new EVENT_LABEL\n",
    "copy and paste the following if you are going this route: \n",
    "\n",
    "</div>\n",
    "\n",
    "```python\n",
    "S3_BUCKET = '2022-data'\n",
    "S3_FILE_IN   = 'rcf_110k.csv'\n",
    "S3_FILE_OUT   = 'rcf_110k_preparedcsv'\n",
    "S3_LOCATION = 's3://{}/{}'.format(S3_BUCKET, S3_FILE_IN)\n",
    "S3_LOCATION_OUT = 's3://{}/{}'.format(S3_BUCKET, S3_FILE_OUT)\n",
    "print(\"input file location: {}\".format(S3_LOCATION))\n",
    "print(\"input file location: {}\".format(S3_LOCATION_OUT))\n",
    "df = pd.read_csv(S3_LOCATION)\n",
    "df.head()\n",
    "\n",
    "````\n",
    "\n",
    "\n",
    "### Dataset Format \n",
    "\n",
    "Assuming we are using Amazon Fraud Detector, your file may or may not contain an EVENT_LABEL but should contain an EVENT_TIMESTAMP. We will be creating an EVENT_LABEL based on the RCF score below. IF you do have an EVENT_LABEL in the data we'll use this to evaluate how well the RCF performs on your sample data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "160875d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input file location: s3://2022-data/rcf_110k.csv\n",
      "input file location: s3://2022-data/rcf_110k_prepared.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>account_tenure_yrs</th>\n",
       "      <th>normalized_currency_amt</th>\n",
       "      <th>original_currency_amt</th>\n",
       "      <th>currency</th>\n",
       "      <th>EVENT_TIMESTAMP</th>\n",
       "      <th>ip_address</th>\n",
       "      <th>email_address</th>\n",
       "      <th>EVENT_LABEL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.0</td>\n",
       "      <td>38334.0</td>\n",
       "      <td>36466.0</td>\n",
       "      <td>eur</td>\n",
       "      <td>2021-08-14 05:38:58</td>\n",
       "      <td>57.115.15.248</td>\n",
       "      <td>lcarlson@example.org</td>\n",
       "      <td>legit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.0</td>\n",
       "      <td>50104.0</td>\n",
       "      <td>24775.0</td>\n",
       "      <td>usd</td>\n",
       "      <td>2021-10-21 13:29:39</td>\n",
       "      <td>203.218.41.214</td>\n",
       "      <td>brianfowler@example.org</td>\n",
       "      <td>legit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16.0</td>\n",
       "      <td>21516.0</td>\n",
       "      <td>46364.0</td>\n",
       "      <td>usd</td>\n",
       "      <td>2021-06-18 03:28:47</td>\n",
       "      <td>131.7.35.71</td>\n",
       "      <td>lthomas@example.org</td>\n",
       "      <td>legit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.0</td>\n",
       "      <td>50446.0</td>\n",
       "      <td>23668.0</td>\n",
       "      <td>usd</td>\n",
       "      <td>2021-05-08 22:44:29</td>\n",
       "      <td>150.245.11.96</td>\n",
       "      <td>victoria20@example.com</td>\n",
       "      <td>legit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.0</td>\n",
       "      <td>43560.0</td>\n",
       "      <td>32309.0</td>\n",
       "      <td>eur</td>\n",
       "      <td>2021-08-24 00:17:08</td>\n",
       "      <td>155.110.112.38</td>\n",
       "      <td>amandamartinez@example.com</td>\n",
       "      <td>legit</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   account_tenure_yrs  normalized_currency_amt  original_currency_amt  \\\n",
       "0                11.0                  38334.0                36466.0   \n",
       "1                 6.0                  50104.0                24775.0   \n",
       "2                16.0                  21516.0                46364.0   \n",
       "3                 6.0                  50446.0                23668.0   \n",
       "4                10.0                  43560.0                32309.0   \n",
       "\n",
       "  currency      EVENT_TIMESTAMP      ip_address               email_address  \\\n",
       "0      eur  2021-08-14 05:38:58   57.115.15.248        lcarlson@example.org   \n",
       "1      usd  2021-10-21 13:29:39  203.218.41.214     brianfowler@example.org   \n",
       "2      usd  2021-06-18 03:28:47     131.7.35.71         lthomas@example.org   \n",
       "3      usd  2021-05-08 22:44:29   150.245.11.96      victoria20@example.com   \n",
       "4      eur  2021-08-24 00:17:08  155.110.112.38  amandamartinez@example.com   \n",
       "\n",
       "  EVENT_LABEL  \n",
       "0       legit  \n",
       "1       legit  \n",
       "2       legit  \n",
       "3       legit  \n",
       "4       legit  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S3_BUCKET = '2022-data'\n",
    "S3_FILE_IN   = 'rcf_110k.csv'\n",
    "S3_FILE_OUT   = 'rcf_110k_prepared.csv'\n",
    "S3_LOCATION = 's3://{}/{}'.format(S3_BUCKET, S3_FILE_IN)\n",
    "S3_LOCATION_OUT = 's3://{}/{}'.format(S3_BUCKET, S3_FILE_OUT)\n",
    "print(\"input file location: {}\".format(S3_LOCATION))\n",
    "print(\"input file location: {}\".format(S3_LOCATION_OUT))\n",
    "df = pd.read_csv(S3_LOCATION)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fc780f",
   "metadata": {},
   "source": [
    "## Profile the data. \n",
    "-----\n",
    "\n",
    "The following function simply profiles your data frame and determines which variables shoudl be included and how they should be handled for the RCF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae71330d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_profile(df):\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    numeric_features = [ c for c in df.select_dtypes(include = np.number).columns ]\n",
    "    \n",
    "    categorical_features = [ c for c in df.select_dtypes(include = np.object).columns ]\n",
    "   \n",
    "    rowcnt = len(df)\n",
    "    df_s1  = df.agg(['count', 'nunique','mean','min','max']).transpose().reset_index().rename(columns={\"index\":\"feature_name\"}).round(4)\n",
    "    df_s1['count'] = df_s1['count'].astype('int64')\n",
    "    df_s1['nunique'] = df_s1['nunique'].astype('int64')\n",
    "    df_s1[\"null\"] = (rowcnt - df_s1[\"count\"]).astype('int64')\n",
    "    df_s1[\"not_null\"] = rowcnt - df_s1[\"null\"]\n",
    "    df_s1[\"null_pct\"] = df_s1[\"null\"] / rowcnt\n",
    "    df_s1[\"nunique_pct\"] = df_s1['nunique'] / rowcnt\n",
    "    dt = pd.DataFrame(df.dtypes).reset_index().rename(columns={\"index\":\"feature_name\", 0:\"dtype\"})\n",
    "    df_stats = pd.merge(dt, df_s1, on='feature_name', how='inner')\n",
    "\n",
    "    df_stats = df_stats.sort_values(\"dtype\").reset_index(drop=True)\n",
    "    # -- part 2 \n",
    "    df_stats['feature_type'] = \"IGNORE\"\n",
    "    df_stats.loc[(df_stats[\"dtype\"] == object) & (df_stats[\"nunique\"] <= 32), 'feature_type'] = \"CATEGORY\"\n",
    "    df_stats.loc[(df_stats[\"dtype\"] == \"int64\") | (df_stats[\"dtype\"] == \"float64\"), 'feature_type'] = \"NUMERIC\"\n",
    "    # df_stats.loc[df_stats[\"dtype\"] == \"float64\", 'feature_type'] = \"NUMERIC\"\n",
    "    df_stats.loc[df_stats[\"feature_name\"] == \"EVENT_LABEL\", 'feature_type'] = \"TARGET\"\n",
    "    df_stats.loc[df_stats[\"feature_name\"] == \"EVENT_TIMESTAMP\", 'feature_type'] = \"EVENT_TIMESTAMP\"\n",
    "    \n",
    "    \n",
    "    # -- part 3 \n",
    "    df_stats['feature_warning'] = \"NO WARNING\"\n",
    "    df_stats.loc[df_stats[\"nunique_pct\"] > 0.7,'feature_warning' ] = \"EXCLUDE, GT 70% UNIQUE\"\n",
    "    df_stats.loc[(df_stats[\"null_pct\"] > 0.2) & (df_stats[\"null_pct\"] <= 0.5), 'feature_warning' ] = \"NULL WARNING, GT 20% MISSING\"\n",
    "    df_stats.loc[df_stats[\"null_pct\"] > 0.5,'feature_warning' ] = \"EXCLUDE, GT 50% MISSING\"\n",
    "    #if null_pct > 0.2 then feature_warning = \"NULL WARNING, GT 20% MISSING\"\n",
    "    df_stats.loc[((df_stats['dtype'] == \"int64\" ) | (df_stats['dtype'] == \"float64\" ) ) & (df_stats['nunique'] < 0.2), 'feature_warning' ] = \"LIKELY CATEGORICAL, NUMERIC w. LOW CARDINALITY\"\n",
    "    #df_stats.loc[((df_stats['dtype'] == \"int64\" ) | (df_stats['dtype'] == \"float64\" ) ) & (df_stats['nunique'] < 0.2), 'feature_warning' ] = \"LIKELY CATEGORICAL, NUMERIC w. LOW CARDINALITY\"\n",
    "    df_stats.loc[df_stats[\"feature_name\"] == \"EVENT_LABEL\",'feature_warning' ] = \"TARGET VARIABLE\"\n",
    "    return df_stats\n",
    "\n",
    "df_stats = df_profile(df)\n",
    "df_stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65b6277",
   "metadata": {},
   "source": [
    "## Data Transformation \n",
    "-----\n",
    "RCF only works on numeric data so we are going to identify which variables we want to use based on the profile and how we'll handle nulls and categorical variables. You can ooptionally overwrite the selection.\n",
    "\n",
    "- numeric variables with null values will be replaced with -1 \n",
    "- categorical variables with null values will be replaced with \"UNKNOWN\"\n",
    "- categorical variables will be dummy encoded for simplicy\n",
    "\n",
    "> Note: only categories with fewer than 32 levels are included, there are of course several other transformations we could perform but this will get the ball rolling!\n",
    "\n",
    "----\n",
    "If you want to include or exclude varaibles in the detection simply specify the variable names in a list like this \n",
    "\n",
    "```python\n",
    "\n",
    "numeric_features = ['account_tenure_yrs', 'normalized_currency_amt', 'original_currency_amt']\n",
    "categorical_features = ['currency']\n",
    "\n",
    "```\n",
    "\n",
    "finally, we are going to check your file to see if it contains an EVENT label if it does pleas identity the \"fraud\" label, we'll perserve the frauds and use them to assess your RCF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ff27d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = df_stats[df_stats[\"feature_type\"] == \"NUMERIC\"][\"feature_name\"].tolist()\n",
    "numeric_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b63a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = df_stats[df_stats[\"feature_type\"] == \"CATEGORY\"][\"feature_name\"].tolist()\n",
    "categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbe0ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_label_ind = df_stats.query('feature_name == \"EVENT_LABEL\"').shape[0]\n",
    "if event_label_ind == 0:\n",
    "    print(\"your dataset doesn't contain a EVENT_LABEL, no problem this will create one for you\")\n",
    "if event_label_ind == 1:\n",
    "    print(\"your dataset does contain EVENT_LABEL, we can perserve your labeled events below\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4569038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- define your fraud label IF you have an EVENT_LABEL!\n",
    "FRAUD_LABEL = \"fraud\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b088ed",
   "metadata": {},
   "source": [
    "## Prepare Data \n",
    "\n",
    "RandomCutForest only deals with numeric data that has no missing values, so we have to deal with missing values and have to dummy encode categories easy way to do this is using panda's get_dummies. \n",
    "\n",
    "---- \n",
    "\n",
    "```python\n",
    "df = pd.get_dummies(df,columns=['currency'])\n",
    "```\n",
    "\n",
    "finally we are going to split the data into a training and test split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf87266",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_dataset(df):\n",
    "    df = df.copy()\n",
    "    df[numeric_features] = df[numeric_features].fillna(-1)\n",
    "    df[categorical_features] = df[categorical_features].fillna(\"UNKNOWN\")\n",
    "   \n",
    "    if event_label_ind == 0:\n",
    "        df_prep = pd.get_dummies(df[numeric_features + categorical_features ], columns=categorical_features)\n",
    "    if event_label_ind == 1:\n",
    "        df_prep = pd.get_dummies(df[[\"EVENT_LABEL\"] + numeric_features + categorical_features ], columns=categorical_features)\n",
    "\n",
    "    return df_prep\n",
    "\n",
    "df_prep = prep_dataset(df)\n",
    "df_prep.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69449ad4",
   "metadata": {},
   "source": [
    "### Review training features\n",
    "-----\n",
    "Before kicking off training, lets double check our input features! \n",
    "\n",
    "you can override the feature selection using the following \n",
    "\n",
    "```python\n",
    "training_features = ['account_tenure_yrs', 'normalized_currency_amt', 'original_currency_amt', 'currency_cad']\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2a6ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will set the input features for training. \n",
    "excluded_features = [\"EVENT_LABEL\",\"EVENT_TIMESTAMP\"]\n",
    "training_features  = [item for item in df_prep.columns if item not in excluded_features]\n",
    "print(\"training input features: \\n{}\".format(training_features))\n",
    "\n",
    "# feature count will give us the dimensionality of our numpy array. \n",
    "feature_count = df_prep.shape[1] - 1\n",
    "print(\"number input features: {}\".format(feature_count))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c614a3f3",
   "metadata": {},
   "source": [
    "### Write Training data to S3\n",
    "\n",
    "---- \n",
    "\n",
    "here we are going to write our prepared data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923fa229",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"number of train instances : {} \".format(df_prep.shape[0] ))\n",
    "# upload train and test to S3 \n",
    "s3 = boto3.client(\"s3\")\n",
    "csv_buf = StringIO()\n",
    "# -- write training data set to S3\n",
    "train_key = 'rcf_train.csv'\n",
    "df.to_csv(csv_buf, header=True, index=False)\n",
    "csv_buf.seek(0)\n",
    "resp = s3.put_object(Bucket=S3_BUCKET, Body=csv_buf.getvalue(), Key=train_key)\n",
    "print(\"writing train to s3 bucket: {}\".format(S3_BUCKET))\n",
    "print(\"{}\\n\".format(resp))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1fb569",
   "metadata": {},
   "source": [
    "## Train your RandomCutForest \n",
    "\n",
    "The following will kick off a fit of RandomCutForest \n",
    "you can tinker with the following parameters \n",
    "\n",
    "num_samples_per_tree - this is the number of samples used to train each tree if there is little variation in the data 512 is fine, larger for more variation.  \n",
    "\n",
    "\n",
    "- num_trees - number of randomly constructed trees. Increasing num_trees has the effect of reducing the noise observed in anomaly scores since the final score is the average of the scores reported by each tree. It is recommend to start with 100 trees as a balance between score noise and model complexity. \n",
    "\n",
    "- num_samples_per_tree - this is the number of samples used to train each tree. num_samples_per_tree is related to the '''expected density''' of anomalies in the dataset. In particular, num_samples_per_tree should be chosen such that 1/num_samples_per_tree approximates the ratio of anomalous data to normal data. For example, if 256 samples are used in each tree then we expect our data to contain anomalies 1/256 or approximately 0.4% of the time. Again, an optimal value for this hyperparameter is dependent on the application.\n",
    "\n",
    "\n",
    "NOTE: RandomCutForest takes a numpy array as training data, so you'll want to convert to an array and reshape the array to number of records (-1) and width of numeric columns (3) for this dataset. if you add dummies for currency for example you'll have 3 currencies of 0s and 1s. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b31366",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import RandomCutForest\n",
    "\n",
    "session = sagemaker.Session()\n",
    "\n",
    "# specify general training job information\n",
    "rcf = RandomCutForest(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m4.xlarge\",\n",
    "    data_location=f\"s3://{S3_BUCKET}/\",\n",
    "    output_path=f\"s3://{S3_BUCKET}/output\",\n",
    "    num_samples_per_tree=512,               # -- change 512, 1024 or some other fraction of file size \n",
    "    num_trees=100,                          # -- number of trees, more is generally good 50 - 1k is generally fine. \n",
    ")\n",
    "\n",
    "# automatically upload the training data to S3 and run the training job\n",
    "rcf.fit(rcf.record_set(df_prep[training_features].to_numpy().reshape(-1,feature_count)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9979db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- deploys model \n",
    "rcf_inference = rcf.deploy(initial_instance_count=1, instance_type=\"ml.m4.xlarge\")\n",
    "rcf_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f91d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- name of the endpiont \n",
    "print(f\"Endpoint name: {rcf_inference.endpoint}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10323e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- setup for predictions \n",
    "from sagemaker.serializers import CSVSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "rcf_inference.serializer = CSVSerializer()\n",
    "rcf_inference.deserializer = JSONDeserializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175500ef",
   "metadata": {},
   "source": [
    "## Evaluate \n",
    "\n",
    "-----\n",
    "\n",
    "So here we are going to score the dataset and evaluate a \"suggested\" outlier score threshold. this will ultimately be used to create our new \"EVENT_LABEL\" \n",
    "\n",
    "1. first score the entire dataset \n",
    "2. identify a \"sugested outlier score threshold\"  based on identifying the top 400 most anomalous events.  \n",
    "3. Finally we'll look at the number of \"anomalies\" produced at the \"sugested outlier_score threshold\", you can simply increase or decrease the threshold to generate the number of anomalies.  \n",
    "\n",
    "> Once you are satisfied with the sugested_threshold, it will be used to create your EVENT_LABEL.\n",
    "\n",
    "you can override the sugestion by simply plugging in a new value for example\n",
    "```python\n",
    "\n",
    "RCF_ANOMALY_THRESHOLD = 3.0\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d019d5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- 1. score entire datset \n",
    "scored_data = df_prep[training_features].to_numpy().reshape(-1, feature_count)\n",
    "results = rcf_inference.predict(scored_data)\n",
    "scores = [datum[\"score\"] for datum in results[\"scores\"]]\n",
    "\n",
    "# add scores to taxi data frame and print first few values\n",
    "df_prep[\"outlier_score\"] = pd.Series(scores, df_prep.index)\n",
    "df_prep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eccde63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- 2. sugested score threshold --- \n",
    "RCF_ANOMALY_THRESHOLD = df_prep.sort_values(\"outlier_score\", ascending=False).iloc[400][\"outlier_score\"]\n",
    "print(\"sugested outlier_score threshold {:.3f}\".format(RCF_ANOMALY_THRESHOLD))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c101169d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "plt.figure(figsize = (20,8))\n",
    "sns.histplot(data=df_prep,x=\"outlier_score\", bins=100)\n",
    "plt.title(\"Predicted Score Distribution - Suggested Threshold {:.3f}\".format(sugested_threshold))\n",
    "plt.xlabel(\"Predicted Score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.axvline(x=RCF_ANOMALY_THRESHOLD, ls='--', linewidth = 2, c='grey')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c8a54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- pick a threshold here i use 2, docs often recomend 3 key is to identify enough variables to train a model \n",
    "\n",
    "if event_label_ind == 0:\n",
    "    df_prep[\"EVENT_LABEL\"]  = np.where(df_prep[\"outlier_score\"] >= RCF_ANOMALY_THRESHOLD, \"fraud\",\"legit\")\n",
    "    print(df[\"EVENT_LABEL\"].value_counts())\n",
    "\n",
    "if event_label_ind == 1:\n",
    "    df_prep[\"EVENT_LABEL\"] = np.where((df_prep[\"outlier_score\"] >= RCF_ANOMALY_THRESHOLD) & (df_prep[\"EVENT_LABEL\"] != FRAUD_LABEL),\"fraud\",\"legit\")\n",
    "    print(\"Original fraud labels + some overlapping number of Anomalies\")\n",
    "    print(df_prep[\"EVENT_LABEL\"].value_counts())    \n",
    "    \n",
    "\n",
    "plt.figure(figsize = (20,8))\n",
    "sns.histplot(data=df_prep,x=\"outlier_score\", bins=100, )\n",
    "plt.title(\"Predicted Score Distribution - Pick a Threshold\")\n",
    "plt.xlabel(\"Predicted Score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xlim(df_prep[\"outlier_score\"].min(), df_prep[\"outlier_score\"].max())\n",
    "plt.text(x=RCF_ANOMALY_THRESHOLD - 0.5,y=3000,s=\"Predicted LEGIT\", fontsize=15)\n",
    "plt.text(x=RCF_ANOMALY_THRESHOLD + 0.25,y=3000,s=\"Predicted FRAUD\", fontsize=15)\n",
    "plt.axvline(x=RCF_ANOMALY_THRESHOLD, ls='--', linewidth = 2, c='grey')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bfa0c3",
   "metadata": {},
   "source": [
    "## Write write out Dataset to S3\n",
    "----\n",
    "1. update/create an EVENT LABEL on the original input dataset. \n",
    "2. write data frame to S3. \n",
    "3. fire up AFD and train a new model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6769fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. drop 'outlier_score' and 'outlier_ind' from the dataset! \n",
    "df[\"EVENT_LABEL\"] = df_prep[\"EVENT_LABEL\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e004aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload train and test to S3 \n",
    "s3 = boto3.client(\"s3\")\n",
    "csv_buf = StringIO()\n",
    "# -- write training data set to S3\n",
    "df.to_csv(csv_buf, header=True, index=False)\n",
    "csv_buf.seek(0)\n",
    "resp = s3.put_object(Bucket=S3_BUCKET, Body=csv_buf.getvalue(), Key=S3_FILE_OUT)\n",
    "print(\"AFD ready file writen to :{}\\n\".format(S3_LOCATION_OUT))\n",
    "print(\"{}\\n\".format(resp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e034896b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
